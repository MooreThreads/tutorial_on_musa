import gradio as gr
import gradio_musa
import time  # âœ… è®¡æ—¶
from openai import OpenAI
import os


model_path="/workspace/qwq-32b_mtt"
print(model_path)
infer_url="http://10.10.142.39:22019/v1"
print(infer_url)

# âœ… åˆ›å»º OpenAI å®¢æˆ·ç«¯
client = OpenAI(base_url=infer_url, api_key="EMPTY")
# âœ… å¤„ç†å¯¹è¯è¯·æ±‚ï¼ˆæ”¯æŒæµå¼ + è®¡ç®—æ¨ç†é€Ÿåº¦ï¼‰
def chat_with_openai(user_input, history):
    history = history or []  # åˆå§‹åŒ–èŠå¤©è®°å½•
    messages =[]
    # âœ… æ„é€  messages ç»“æ„
    messages = [{"role": "system", "content": "You are a chatbot. You can help people with their questions about anything. Always enclose latex snippets with dollar signs! For example, $$\phi$$"}]
    for user_msg, bot_msg in history:
        messages.append({"role": "user", "content": user_msg})
        answer = bot_msg[bot_msg.find("[ğŸ’­æ€è€ƒå®Œæ¯•]") + len("[ğŸ’­æ€è€ƒå®Œæ¯•]"):]
        messages.append({"role": "assistant", "content": answer})


    messages.append({"role": "user", "content": user_input})
    print(messages)

    # âœ… è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()
    token_count = 0  # âœ… è®°å½•ç”Ÿæˆçš„ Token æ•°é‡

    # âœ… å‘é€è¯·æ±‚
    try:
        response = client.chat.completions.create(
            model=model_path, # /data/mtt/models_convert/DeepSeek-R1-Distill-Llama-70B-converted", # converted_model_dir,  # âœ… ä½ çš„æ¨¡å‹è·¯å¾„
            messages=messages,
            stream=True  # âœ… å¯ç”¨æµå¼è¾“å‡º
        )

        bot_response = ""
        for chunk in response:
            chunk_data = chunk.model_dump()  # âœ… è§£æå“åº”æ•°æ®
            if "choices" in chunk_data and len(chunk_data["choices"]) > 0:
                delta = chunk_data["choices"][0].get("delta", {})
                content = delta.get("content", "")
                if content:
                    bot_response += content
                    bot_response = bot_response.replace('<think>', '[ğŸ’­å¼€å§‹æ€è€ƒ]')
                    bot_response = bot_response.replace('</think>', '[ğŸ’­æ€è€ƒå®Œæ¯•]')
                    token_count += 1  # âœ… æ¯ä¸ª Token è®¡æ•°
                    yield history + [(user_input, bot_response)], "", "æ¨ç†ä¸­..."

        # âœ… è®°å½•ç»“æŸæ—¶é—´ & è®¡ç®—æ—¶é•¿
        elapsed_time = time.time() - start_time
        tps = token_count / elapsed_time if elapsed_time > 0 else 0  # âœ… è®¡ç®— Tokens Per Second

        speed_text = f"â±ï¸  è€—æ—¶: {elapsed_time:.2f} ç§’ | ğŸ”¢ Tokens: {token_count} | âš¡ é€Ÿåº¦: {tps:.2f} TPS"

        yield history + [(user_input, bot_response)], "", speed_text  # âœ… è¿”å›æ¨ç†é€Ÿåº¦

    except Exception as e:
        yield history + [(user_input, f"âŒ è¯·æ±‚å¤±è´¥: {e}")], "", "â±ï¸  æ¨ç†å¤±è´¥"

# âœ… æ¸…é™¤èŠå¤©è®°å½• & è®¡æ—¶å™¨
def clear_chat():
    return [], "", "â±ï¸  è€—æ—¶: 0.00 ç§’ | ğŸ”¢ Tokens: 0 | âš¡ é€Ÿåº¦: 0.00 TPS"  # âœ… æ¸…ç©ºæ‰€æœ‰ UI

# âœ… åˆ›å»º Gradio ç•Œé¢
with gradio_musa.Blocks() as demo:
    #gr.Markdown("### ğŸ’¬ Deepseek-R1-Distill-Llama-70B æ¨ç†æ¨¡å‹")

    chatbot = gr.Chatbot(label="Running on MTT S4000")  # âœ… æ˜¾ç¤ºå¯¹è¯å†å²
    msg_input = gr.Textbox(label="è¾“å…¥ä½ çš„é—®é¢˜", placeholder="è¯·è¾“å…¥...", lines=1, autofocus=True)  # âœ… æ”¯æŒ>å¤šè¡Œè¾“å…¥
    speed_display = gr.Textbox(label="æ¨ç†é€Ÿåº¦", value="â±ï¸  è€—æ—¶: 0.00 ç§’ | ğŸ”¢ Tokens: 0 | âš¡ é€Ÿåº¦: 0.00 TPS", interactive=False)  # >âœ… æ˜¾ç¤ºæ¨ç†é€Ÿåº¦

    with gr.Row():
        submit_btn = gr.Button(value="æäº¤")
        clear_btn = gr.Button("æ¸…é™¤å†å²")  # âœ… æ·»åŠ æ¸…é™¤æŒ‰é’®

    # âœ… ç»‘å®šäº‹ä»¶
    msg_input.submit(chat_with_openai, inputs=[msg_input, chatbot], outputs=[chatbot, msg_input, speed_display])  # âœ… æŒ‰ Enter è§¦å‘
    submit_btn.click(chat_with_openai, inputs=[msg_input, chatbot], outputs=[chatbot, msg_input, speed_display])  # âœ… æŒ‰æŒ‰é’®è§¦å‘
    clear_btn.click(clear_chat, inputs=[], outputs=[chatbot, msg_input, speed_display])  # âœ… æ¸…é™¤èŠå¤© & è®¡æ—¶

demo.queue()  # âœ… å…è®¸æµå¼æ•°æ®ä¼ è¾“
demo.launch(server_name="0.0.0.0")  # âœ… å¯åŠ¨ Gradio ç•Œé¢
